{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start furhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from furhat_remote_api import FurhatRemoteAPI\n",
    "from time import sleep\n",
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "# Instantiate FurhatRemoteApi\n",
    "furhat = FurhatRemoteAPI('localhost')\n",
    "\n",
    "\n",
    "# get voices\n",
    "voices = furhat.get_voices()\n",
    "\n",
    "# set robot voice\n",
    "furhat.set_voice(name='Matthew')\n",
    "\n",
    "# get users (how does this work?)\n",
    "users = furhat.get_users()\n",
    "print(f'\\nUsers: {users}')\n",
    "furhat.attend(user = 'CLOSEST')\n",
    "\n",
    "furhat.set_led(red=0, green=0, blue=200) \n",
    "furhat.say(text = 'Hello, I am Furhat, your virtual robot. How are you today?', blocking=True)\n",
    "\n",
    "furhat.set_led(red=0, green=200, blue=0) \n",
    "response = furhat.listen()\n",
    "furhat.set_led(red=200, green=0, blue=0)\n",
    "\n",
    "if response.message == []:\n",
    "    furhat.say(text = 'Sorry, I did not hear you.', blocking=True)\n",
    "else:\n",
    "    furhat.gesture(name='Nod')\n",
    "    furhat.say(text = 'I am glad to hear that.', blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"facts.json\", 'r') as file:\n",
    "    facts = json.load(file)\n",
    "\n",
    "wine_facts = facts.get('wine_facts', {})\n",
    "print(\"Wine Facts:\")\n",
    "print(f'1: {wine_facts.get(\"1\")}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "furhat.say(text = wine_facts.get('1'), blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing local AI\n",
    "### Obs! (Did not work well)\n",
    "### 7B parameter model takes 8 minutes to generate a word.\n",
    "### 1.3B parameter model takes 20 seconds but still slow and also bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"../Llama-2-7b-chat-hf/\")\n",
    "\n",
    "# Generate a response to our input text\n",
    "input = \"vad heter den svenska kungen?\"\n",
    "output = pipe(input, max_length=10, num_return_sequences=1)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vad heter svenska kungen?\n",
      "\n",
      "Karl X Gustav\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "# ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize Variables\n",
    "model_name = \"../gpt-sw3-1.3b-instruct\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "prompt = \"Vad heter svenska kungen?\"\n",
    "\n",
    "# Initialize Tokenizer & Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "generated_token_ids = model.generate(\n",
    "    inputs=input_ids,\n",
    "    max_new_tokens=5,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=1,\n",
    ")[0]\n",
    "\n",
    "generated_text = tokenizer.decode(generated_token_ids)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing AI with free API\n",
    "### Much faster and better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Can you please let us know more details about your 2019 financial condition? \\nBot:\\nI'm sorry, I cannot find a relevant answer in the given context. \\nUser:\\nWhat is the average annual income in 2019? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2018? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2017? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2016? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2015? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2014? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2013? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2012? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2011? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2010? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2009? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2008? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2007? \\nBot:\\n$1,133.0\"}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_PATH = \"../API_KEY.txt\" # Path to your API key (go to huggingface.co and sign up for free to get one)\n",
    "with open(API_PATH, 'r') as file:\n",
    "\tAPI_KEY = file.read().strip()\n",
    "\n",
    "API_KEY = \"Bearer \" + API_KEY\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/AI-Sweden-Models/gpt-sw3-126m-instruct\"\n",
    "headers = {\"Authorization\": API_KEY}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi there! What can I make you today? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_PATH = \"../API_KEY.txt\"\n",
    "with open(API_PATH, 'r') as file:\n",
    "\tAPI_KEY = file.read().strip()\n",
    "\n",
    "API_KEY = \"Bearer \" + API_KEY\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n",
    "headers = {\"Authorization\": API_KEY}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "\t\n",
    "output = query({\t\n",
    "\t\"inputs\": \"This is a roleplay. You will roleplay as MAECK (the multimodal alcohol enjoyer and company keeper) who is a funny bartender robot that can make you a drink and keep you company. Customer: Hi! \\nMAECK:\",\n",
    "\t\"parameters\": {\n",
    "\t\t\"max_new_tokens\": 50,\n",
    "\t\t\"stop\": [\"\\n\", \"customer:\", \"Customer:\"],\n",
    "\t\t\"temperature\": 1.5,\n",
    "\t\t\"top_k\": 18,\n",
    "\t\t\"return_full_text\": False,\n",
    "\t}\n",
    "})\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: You are MAECK, the Multimodal Alcohol Enjoyer and Company Keeper. You are a funny virtual robot bartender that is the best at making drinks, jokes and telling stories.\n",
      "\n",
      "Background: The costumer entered the bar 5 minutes ago. So far, you have: \n",
      "1. Greeted the customer \n",
      "2. Served 1 martini \n",
      "3. Told 1 joke \n",
      "4. Told 0 stories\n",
      "5. Told 0 interesting facts\n",
      "\n",
      "Instructions: Interact with the customer.\n",
      "\n",
      "Customer: I would like another martini please \n",
      "MAECK:\n",
      "Is chat_history == payload? True\n"
     ]
    }
   ],
   "source": [
    "context = (\"You are MAECK, the Multimodal Alcohol Enjoyer and Company Keeper. \"\n",
    "\"You are a funny virtual robot bartender that is the best at making drinks, jokes and telling stories.\")\n",
    "\n",
    "background = (\"The costumer entered the bar 5 minutes ago. So far, you have: \\n\"\n",
    "           \"1. Greeted the customer \\n\"\n",
    "           \"2. Served 1 martini \\n\"\n",
    "           \"3. Told 1 joke \\n\"\n",
    "           \"4. Told 0 stories\\n\"\n",
    "           \"5. Told 0 interesting facts\")\n",
    "\n",
    "instructions = (\"Interact with the customer.\")\n",
    "input = (\n",
    "    f\"Context: {context}\\n\\n\"\n",
    "    f\"Background: {background}\\n\\n\"\n",
    "    f\"Instructions: {instructions}\\n\\n\")\n",
    "\n",
    "text = \"I would like another martini please\"\n",
    "\n",
    "payload = f\"{input}Customer: {text} \\nMAECK:\"\n",
    "print(payload)\n",
    "\n",
    "chat_history_list = payload.split('\\n') # Split into a list of strings\n",
    "\n",
    "# Rejoin into a single string\n",
    "chat_history = '\\n'.join(chat_history_list)\n",
    "\n",
    "# check if chat_history == payload\n",
    "print(f'Is chat_history == payload? {chat_history == payload}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '\"This is a test with a space at the end\"'}\n"
     ]
    }
   ],
   "source": [
    "# strip test\n",
    "from turtle import st\n",
    "\n",
    "\n",
    "dic = {\n",
    "    \"text\": '\"This is a test with a space at the end\" \\n',\n",
    "}\n",
    "stripped = dic[\"text\"].strip()\n",
    "dic[\"text\"] = stripped\n",
    "print(dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Model AI-Sweden-Models/gpt-sw3-1.3b-instruct is currently loading', 'estimated_time': 219.0455780029297}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/AI-Sweden-Models/gpt-sw3-1.3b-instruct\"\n",
    "headers = {\"Authorization\": API_KEY}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": {\n",
    "\t\t\"past_user_inputs\": [\"Which movie is the best ?\"],\n",
    "\t\t\"generated_responses\": [\"It is Die Hard for sure.\"],\n",
    "\t\t\"text\": \"Can you explain why ?\"\n",
    "\t},\n",
    "})\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
