{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start furhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from furhat_remote_api import FurhatRemoteAPI\n",
    "from time import sleep\n",
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "# Instantiate FurhatRemoteApi\n",
    "furhat = FurhatRemoteAPI('localhost')\n",
    "\n",
    "\n",
    "# get voices\n",
    "voices = furhat.get_voices()\n",
    "\n",
    "# set robot voice\n",
    "furhat.set_voice(name='Matthew')\n",
    "\n",
    "# get users (how does this work?)\n",
    "users = furhat.get_users()\n",
    "print(f'\\nUsers: {users}')\n",
    "furhat.attend(user = 'CLOSEST')\n",
    "\n",
    "furhat.set_led(red=0, green=0, blue=200) \n",
    "furhat.say(text = 'Hello, I am Furhat, your virtual robot. How are you today?', blocking=True)\n",
    "\n",
    "furhat.set_led(red=0, green=200, blue=0) \n",
    "response = furhat.listen()\n",
    "furhat.set_led(red=200, green=0, blue=0)\n",
    "\n",
    "if response.message == []:\n",
    "    furhat.say(text = 'Sorry, I did not hear you.', blocking=True)\n",
    "else:\n",
    "    furhat.gesture(name='Nod')\n",
    "    furhat.say(text = 'I am glad to hear that.', blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"facts.json\", 'r') as file:\n",
    "    facts = json.load(file)\n",
    "\n",
    "wine_facts = facts.get('wine_facts', {})\n",
    "print(\"Wine Facts:\")\n",
    "print(f'1: {wine_facts.get(\"1\")}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "furhat.say(text = wine_facts.get('1'), blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing local AI\n",
    "### Obs! (Did not work well)\n",
    "### 7B parameter model takes 8 minutes to generate a word.\n",
    "### 1.3B parameter model takes 20 seconds but still slow and also bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1c4180f64a4d8fab030126fb7627d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Unterscheidung von\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"../Llama-2-7b-chat-hf/\")\n",
    "\n",
    "# Generate a response to our input text\n",
    "input = \"Hi\"\n",
    "output = pipe(input, max_length=3, num_return_sequences=1)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vad heter svenska kungen?\n",
      "\n",
      "Karl XIV\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "# ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize Variables\n",
    "model_name = \"../gpt-sw3-1.3b-instruct\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "prompt = \"Vad heter svenska kungen?\"\n",
    "\n",
    "# Initialize Tokenizer & Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "generated_token_ids = model.generate(\n",
    "    inputs=input_ids,\n",
    "    max_new_tokens=5,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=1,\n",
    ")[0]\n",
    "\n",
    "generated_text = tokenizer.decode(generated_token_ids)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing AI with free API\n",
    "### Much faster and better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Can you please let us know more details about your 2019 financial condition? \\nBot:\\nI'm sorry, I cannot find a relevant answer in the given context. \\nUser:\\nWhat is the average annual income in 2019? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2018? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2017? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2016? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2015? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2014? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2013? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2012? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2011? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2010? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2009? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2008? \\nBot:\\n$1,133.0 \\nUser:\\nWhat is the average annual income in 2007? \\nBot:\\n$1,133.0\"}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_PATH = \"../API_KEY.txt\" # Path to your API key (go to huggingface.co and sign up for free to get one)\n",
    "with open(API_PATH, 'r') as file:\n",
    "\tAPI_KEY = file.read().strip()\n",
    "\n",
    "API_KEY = \"Bearer \" + API_KEY\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/AI-Sweden-Models/gpt-sw3-126m-instruct\"\n",
    "headers = {\"Authorization\": API_KEY}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! What can I serve ya? I make the best drinks in town! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_PATH = \"../API_KEY.txt\"\n",
    "with open(API_PATH, 'r') as file:\n",
    "\tAPI_KEY = file.read().strip()\n",
    "\n",
    "API_KEY = \"Bearer \" + API_KEY\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n",
    "headers = {\"Authorization\": API_KEY}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "\t\n",
    "output = query({\t\n",
    "\t\"inputs\": \"This is a roleplay. You will roleplay as MAECK (the multimodal alcohol enjoyer and company keeper) who is a funny bartender robot that can make you a drink and keep you company. Customer: Hi! \\nMAECK:\",\n",
    "\t\"parameters\": {\n",
    "\t\t\"max_new_tokens\": 50,\n",
    "\t\t\"stop\": [\"\\n\", \"customer:\", \"Customer:\"],\n",
    "\t\t\"temperature\": 1.5,\n",
    "\t\t\"top_k\": 18,\n",
    "\t\t\"return_full_text\": False,\n",
    "\t}\n",
    "})\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Model AI-Sweden-Models/gpt-sw3-1.3b-instruct is currently loading', 'estimated_time': 219.0455780029297}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/AI-Sweden-Models/gpt-sw3-1.3b-instruct\"\n",
    "headers = {\"Authorization\": API_KEY}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": {\n",
    "\t\t\"past_user_inputs\": [\"Which movie is the best ?\"],\n",
    "\t\t\"generated_responses\": [\"It is Die Hard for sure.\"],\n",
    "\t\t\"text\": \"Can you explain why ?\"\n",
    "\t},\n",
    "})\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
